2025-09-21 rl residency notes
-----------------------------

---
> over the past week, i've been doing some preliminary real usage testing for
lean programming capabilities of frontier llms.
---

## I. i'm doing this for the following reasons:
  1. feasibililty of using frontier lms as a source of synthetic data.
  2. scoping out which types of problems are suitable for llm lean pair / agent
     programming. both in terms of benchmarks, as well as training tasks.
  3. assessing which tools might be necessary in downstream environment design.

---

## II. i've been using llms to write a non trivial piece of software (in this case, a compiler), to sniff out:

  1. awareness of functional design patterns in lean.
  2. ability to reason and adapt those patterns.
  3. understanding of underlying semantics and implementation of the language.
  4. awareness of the broader lean ecosystem, particularly as it relates to ability
  to competently use the language.

i tested opus 4.1, gemini 2.5 pro and grok 4 expert for this task (but
primarily claude). i didn't try gpt 5 because i don't have gpt pro, but also
because i don't know how to wrap my head around the inferencing meta that that
'model' is going for.

---

III. here are some findings:
  1. all of the tested models have preliminarhy understanding of the lean4 programming
    in cursory prompts. they understand what the language is, roughly how it works,
    and the general syntax should a programming problem be presented.
  2. however, when it comes to actually programming in lean4, only opus 4.1
    really successfully writes compiling programs. grok 4 thinks hard, but doesn't
    seem to actually know the language at all. gemini 2.5 pro understands lean
    quite well.
  3. all models struggle to keep up with the language's developments. the biggest
    hiccup happens whenever a program tries to reach into lean's extended standard
    library. (frankly this is very confusing, the module is called 'Batteries' and
    installing + using it was very non obvious. this is more lean's fault than
    the llms')

---

## IV. what have i realized?

1. using frontier lms (especially claude and gemini) as a source of synthetic
data for lean programming is absolutely feasible. this will be helpful in
the automated construction of prompts, as well as sft data. rejection
sampling on only compiling outputs will be very handy here.

2. it seems that __most__ problems that require only lean, and touches surface
level programming (leetcode / advent of code style) will be completely blitzed
by even high tier oss llm. 

  a. more complicated though is usage that depends on anything that lives
    in the greater lean ecosystem. while this isn't really an issue for the time
    being, this will definitely bite once we try to do nontrivial programming
    tasks (even a basic web programming project will require a correct
    implementation of the http protocol, which might have complicated setup steps
    and usage.)

  b. this suggets that good indexing tools will be crucial if we want good down
    -stream performance. there is https://loogle.lean-lang.org/, but this is
    focused on mathlib related stuff. still ok to use for stdlib, but no go for
    anything else.

3. a huge bottleneck in this project will be my personal skill at vetting lean code
and solving problems in the language. while the compiler pretty helpful feedback,
getting a model unstuck will require pretty substantial knowledge of the language.

> i'm not sure what implications this has for the future of 'lean vibe coding',
 but in the interim, proficiency in lean seems to be a foundational skill check.
 i'll be dedicating around a third of my weekly research hours into getting better
 at the language moving forward.

---

## V. some extras

https://github.com/PrimeIntellect-ai/prime-environments/pull/29 (really
sorry this is taking so long by the way). the major blocker is how arcane
some of the build instructions for the less popular theorem proving languages
are ridiculous (hollight only has as a repl backend, making programmatic access
very difficult ; metamath requires some metaprogramming to link to the 'stdlib'
set.mm files). this to a limited effect also appplies to lean.
-> this suggests that a generic, flexible sdk for programmatically accessing
lean will be crucial downstream.

---

## VI. some expected faq from claude and my answers

1. Did the compiler project reveal specific Lean constructs that consistently
trip up models? (Pattern matching? Type classes? Monads?)

> all models understood pattern matching perfectly. it turns out to be a very
simple construct. same with type classes, and how to programmatically solve
problems with them. i suspect that both of these are due to functional programming
languages sharing this category theoretic 'core' that allows problem solving
tactics to be ported from one to another.
> 
> for monads specifically though, there is finickey syntax in lean4 that sometimes
trips up claude. i haven't tested monadic problem solving with the other models.
>
> all of them completely give up when they run into lean's hard edges (non
termination, and correctness proofs. claude defensively puts in a sorry every
time). fortunately, for non termination, we can always just use partial def and
it works out just fine. i suspect the correctness proofs to be a major blocker
down the line though.

--------------------------------------------------------------------------------

2. The surface-level vs. ecosystem dependency distinction is crucial. Have you
thought about creating a taxonomy of "dependency depth" for problems? Could
help prioritize which benchmarks to build first.

> absolutely. which is why i'll have everything live on depth 0 for the time being,
no complicated runtimes, just flat types and functions on concrete traditional
cs problems like data structures and leetcode style things.


--------------------------------------------------------------------------------

3. Given the Batteries confusion, would it make sense to create a "Lean
environment setup" benchmark as a precursor? Testing if models can even
configure a project correctly before testing if they can code?

> that would be a very interesting idea. though this is more of an 'agent' problem
than specifically a lean programming problem. if they've already managed to
figure out conda, i think this is mostly just a data issue that'll fix itself
as lean gains in poplularity. lakefile.toml files are also exhaustive enough
to describe the entire state of the environment anyhow.
