2025-09-28 rl residency notes
-----------------------------

---
> over the past week, i've been doing some more exploration of lean's quirks as a
language for general software engineering, and writing infra code for
implementing programming benchmarks in lean.
>
> also, sorry for the slow week. i had to wrap up an iclr submission.
---


## I. the sdk tldr:

i wrote a [lightweight sdk](https://github.com/spikedoanz/lean_bench) for
interacting with the lean compiler. it's abstracted over a whole lean project
and makes a couple of assumptions which i find correct at the moment:
- code from a project that is not llm generated is immutable.
- there will be thousands of requests to this compiler.
- all llm code will be in a single standalone file.

so the end result is a largely stateless server process that's trivially scalable,
can handle multiple compilation steps across multiple projects, with exact setup
steps for projects being left up to implementation.

---

## II. why was the sdk designed this way?

  1. cacheability 
  the lean compiler produces intermediary artifacts (.olean files) that
  correspond to source files. importantly: it only does this IF the file passed
  the compiler checks. by making the source immutable, i can use it as a caching
  mechanism, as well as a fallback for verifiying that llm outputted text has
  been verified.

  2. avoiding benchmark hacking
  it's a pretty trivial way to guarantee (by construction, the model does not have
  access to the actual codebase, and only has append access) that the model
  cannot modify the harness.

  3. scalability
  this is designed primarily with throughput and scalability in mind. prelim
  testing on my macbook is getting me 6-8 files a second (a couple hundred
  theorems/functions, if concatenated into the same file). arguably not great,
  but this should in principle natively scale with more hosts.

---

## III. lean is not completely unhackable

lean has a couple of escape hatches that allows a source file to pass the
compiler checks without satisfying all of the spec requirements:
- parital def : allows a function to bypass termination checks
- sorry : allows a theorem to be 'ignored' during a proof step. shouldn't be a
concern given that this requires a special compiler flag.
- admit : alias for sorry. 
- unsafeCast : allows conversion of a type, say from "List" to "SortedList",
silently.

all of these are fortunately trivial to spot in plain lean, but gets more
complicated when external libraries are involved, especially for tactics.

this has been seen before in the deepseek math v2 release
https://arxiv.org/abs/2504.21801, where the model uses a quirk in the
implementation of a handful of niche tactics in order to bypass some proof
steps.

---

## IV. benchmark targets

i plan on converting some of these coding competitions into test harnesses for
the lean environments. this'll take some delicate maneuvering of the read
accesses of the benchmarks and their test cases, but nothing a little elbow
grease can't solve:
- https://adventofcode.com/ : most suitable. widely accessible, high variance
and density of problems.
- https://cses.fi/
- https://projecteuler.net/ 
  - https://github.com/Orbiter/project-euler-llm-benchmark


i'll likely be targetting advent of code first, since it lets me double dip on
personal lean practice, but also because the plethora of apis people have
built on top of the competition will make it much easier to convert into a
benchmark.
