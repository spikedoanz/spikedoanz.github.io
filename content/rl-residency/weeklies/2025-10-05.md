2025-10-05 rl residency notes
-----------------------------

---
> over the past week, i've worked on two core formal programming environments
>
> minif2f (mostly to wrap up my work, and to focus more on the programmatic
side of lean programming)
https://github.com/PrimeIntellect-ai/prime-environments/pull/29
>
> aocbench: a port of the advent of code programming puzzle series
https://github.com/spikedoanz/aocbench.
---


## I. a memo on formal language compilers

> skip to section II if you just want to get to the concrete research
artifacts from this week.

### 1. lean is the best option for now

in wrapping up my work with minif2f, i realized that of the formal languages
used in that benchmark (lean, isabelle, metamath and hollight), lean had the
most mature and reliable toolchain. the other languages had very arcane
installation steps, or were bootstrapped off of other language (hollight 
is 'used' by starting an ocaml repl session and manually pasting in theorems).

i'll spare the details of the other languages, but in short, it was a massive
pain to automate the compilation step of anything that wasn't lean.

it was also very slow. metamath in particular required importing a massive
41MB set.mm file for every verification.

lean by comparison even had a dedicated package for setting up mathlib
https://pypi.org/project/mathlibtools/0.0.3/, so it's obvious that people
have been willing and able to construct an ecosystem for this language.

---

### 2. though that doesn't mean that lean is perfect

if you have been following recent trends on twitter, people (even someone
as presciently early as doomslide) has realized that massively scaling
lean is not going to be an end all be all to llm programming (and especially
math) https://x.com/doomslide/status/1974150581431513557

doomslide definitely has a point w.r.t math (i have no idea how to read what
was in the [trinity abc conjecture
proof](https://github.com/morph-labs/lean-abc-true-almost-always)), and i'm
undecided on what the implications are for code.

for the unaware, lean for math and lean for code are actually two very
different language. syntactically, lean for math heavily use 'tactics',
https://lean-lang.org/doc/reference/latest/Tactic-Proofs/Tactic-Reference/
, which are a pretty sophisticated metaprogramming system for operating
over type terms. by comparison, lean for code is a pretty bog standard
functional programming language. this difference mostly means that
for most programmers, the math side of lean will be more difficult to
read, even if they're proficient functional programmers.

semantically though, they are vastly different for one key reason:
__irrelevance__. proofs in lean are are considered equivalent if they
prove the same thing, regardless of how they're proven. this makes it
vastly more probable that an automatically generated proof in lean is
some uninterpretable garbled nonsense.

---

programs in lean __do not__ have proof irrelevance, which means that we can be
much more specific regarding the semantics of an llm generated codebase.
although, it's still pretty worrisome at the limit case, considering:

a. the semantics of lean itself are not completely formalized in the language,
although projects like [lean4lean](https://arxiv.org/abs/2403.14064) are a step
in the right direction.

b. the lean kernel itself (the language runtime + type checker) is not
formalized, and in principle cannot be formalized due to godel's incompleteness
theorem (a system cannot prove its own consistency).

this means that going forward, trust in the lean kernel will become
increasingly load bearing. furthermore, given the magnitude of work required to
become proficient in understanding the lean kernel's implementation, this
means most researchers will effectively have to put their faith in a small
priesthood of type theorists to guarantee that lean artifacts from llms
are at all trustworthy and applicable, and that lean itself is a worthy
platform on which to scale ~billion of dollars in training, inference and
especially accumulating code artifacts.

---

### 3. what can be done about this uncertainty?

effectively, i believe some promsing course of action, on a personal and broad
research community level will be:

a. proliferation of materials and learning environments for interacting with
formal languages (current and future) : this will greatly help with the
trusted priesthood issue, and given that the languages are open source, make
it far more feasible to spot and patch out errors in the kernel.

b. creation of a vastly simpler formal languages that serve as a better testbed
for 'vibe formal programming' : this is somewhat contingent on point 3.a, but
technically, i don't think it's that much harder than, say, a pytorch
competitor, so there is definitely hope in the technical feasibility of this
kind of project.
roughly equivalent to lean as a target for vibe formal programming, such as
coq, agda, idris2, bend

c. nothing : we're stuck in a place of unknown unknowns regarding the
practicality of using lean as a vibe coding target. and the best way to resolve
these unknown unknowns is to just try it out. one billion lines of lean code,
deployed in controlled settings, is a great way to figure out if something
fundamental is wrong with the language. same thing happened with javascript,
and it seems to be getting better YoY. the shape of the language necessary
for compounding verifiable artifacts will be revealed in grasping for it
using whatever tools we have today, and i think that's something i'm willing
to bet a couple years of research on.


## II. regarding advent of code bench

### 1. the brief

a. the benchmark fully respects eric wastl's copyright over advent of code's
puzzle text and inputs, and has reasonable design decisions for not
stressing their infrastructure. this was done at the cost of pretty aggressive
rate limiting, and requiring a manually fetched advent of code user token.

b. i've ported 2015 and 2016, of which 2015 is well tested (though not much was
 needed). all future years should be trivial given a sample solution (of
which there are many available with MIT licences on github). this means that i
could expand to include the years through 2024 with minimal additional effort.

c. it's not saturated with even gpt 4.1! even when providing the docs of all
required stdlib functions. and not only does it not produce the correct
answer zero shot, it also doesn't produce a compiling answer zero shot.

with this benchmark in place, i'll be able to start doing some sweeps on oss
and frontier models next week, and maybe even kick of a training run or two!

all in all, aocbench alone will provide nearly 500 programming puzzles.

### 2. some extended notes

in addition to being a benchmark, i've designed aocbench in such a way that it
also works as a frontend for doing coding puzzles in lean. every puzzle gets
templated into the project format, with build systems and input handling
already dealt with (reading files is non trivial in lean, if you recall my
previous notes).

this means that it could be used as an educational platform for teaching people
how to write lean! i plan to launch both aocbench as the benchmark, and also
as a 'rustlings for lean' type project next week. this should draw some mindshare
away from lean purely being a math focused language, and more towards pratical
software engineering.


another thing that i realized in creating this benchmark was that i was only
partially right with my design decisions of the lean compiler sdk from last
week's notes. notably:
  - cachability matters far less than i thought, at least at this stage: the
  scale of the compilation jobs current will span much more along the N of
  different environments axis, far moreso than M rollouts within a given
  benchmark.
  - factoring out each task into its own environment is wasteful, but the
  right call for the time being, given the interpretability benefits.

overall, a classic case of premature optimization. though not super wasteful
since i never would've landed on this simpler design if i hadn't put in the
work on lean-bench-sdk.

---

## III. next week's plans

two things:

- launches: i'll announce aocbench right after i do a sweep on the qwens, kimi
k2,gpt 5 pro, opus. i will also do the same right after minif2f gets merged
into prime environments.
  > this will get me to finally dust off the gpu tokens johannes sent
  me 3 weeks ago. better late than never.
  - this will be accompanied by a brief blog post detailing my rl residency
  here at PI, and general research goals for the next couple months.

- early RL loop: 500 puzzles should be plenty enough to kick off a small
training run. i'll evaluate on qwen 14B (smallest effective model trainable on
8xH/A100) and assess whether i'll need sft data from lean docs, or generated
from god models.

overall, proud of the work that i've gotten done this week, and excited for the
unlocks that i obtained.
